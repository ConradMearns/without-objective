I'm kicking off 2026 with a mission: start more internet fights. Come at me in the comments if you think I'm wrong - or find me and buy me a drink. 

In 2019, Richard Sutton - now partnered with John Carmack at Keen Technologies - wrote _The Bitter Lesson_. The main thesis of which is that general methods that leverage computation are the most effective approach in AI research by large margins - and that encoding human knowledge tends to be counter-productive in the long run.

I've been seeing some takes recently that apply this thesis to the current direction of LLM code generation. 

Most frustratingly - the takes are low-to-mid on the spicy scale and don't actually _say_ anything to which I could build a rebuttal for. Most are claiming "how and what we build will change" - which is a horrible prediction considering for most of us - everything already has.

The general vibe of each argument is all that I can attack - that these models are going to continue to scale, and capabilities will grow, until we can finally give these machines real responsibilities instead of todo-lists.

And I disagree. Mostly with the use and attempt to leverage _The Bitter Lesson_ as a qualifier for that warrant.

_The Bitter Lesson_ is primarily about games. It discusses the scale and naive algorithm that allowed DeepBlue to beat Kasparov - and mentions AlphaGo.

While AlphaGo is mentioned - it mis-identifies that the version of AlphaGo that beat Lee Sedol in 2016 _was_ trained on human encoded knowledge. The idea that AlphaGo could learn and play Go without "studying" human expertise was released a year later as AlphaGo Zero.

(I'll let Rich have that one though - his year was only one off. An acceptable error for an incomplete idea;-)

The bet many are making in regards to LLM's however - is that scale will increase intelligence and effective problem solving capability.

This is likely wrong - we are still hedging bets and dissecting models obviously. However, we have good indications that these models _won't_ continue to scale as expected. 

Yusuf Kalyoncuoglu's December 2025 paper explains that "intelligence" only needs an 8-dimensional manifold (instead of ~12k) to be accurately captured and used in distilled models.

What you get with scale are more problems, more untruths, and most importantly _more redundancy.

The power of LLM's lies in prediction, not problem-solving. They can do this because they are _nothing_ except for the sum of terabytes of human expertise smashed into some nonlinear manifold.

LLM's _are_ encoded expertise. They have not learned human language from scratch, they have not invented techniques to "play the game" like AlphaGo Zero. They are still just an impressively vast simulacrum. 

They cannot "learn on the job" - they must be retrained and redeployed.

If the Bitter Lesson holds - it won't hold for LLM's, simply because the concepts connect as well as parallel lines on a plane. We aren't trying to "solve" for language like we "solved" Chess and Go. We still don't even know what problems are best to solve with LLM's in the first place.

We don't know what the game is. We will continue to throw LLM's at the wall and eventually _something_ will stick, but my bet is that the real winners won't be the LLM's vendors - they'll be the LLM distilleries. The folks who understand that the _real_ games we _could_ be playing; breaking down walls, resolving difficult communication, committing to accessability, and reducing the risks of maintenance for not only software, but our legal and political institutions too.

All of which will require _even more human expertise_, not less.

This is because our most difficult problems domains _are_ the domains of human expertise. Unlike Chess, we still don't have an algorithm for managing hospitals and patient care, or water rights management, or global deescalation.

All in all - how we build is already completely different. What we build is still the same, and won't change until perception about "the game" changes. 

_We_ haven't solved Software Development as a game still, so naturally I can't trust that unaligned Large Language Models could either.

My take is that 2026 will made by the knowledge gardeners and technological librarians who aim to leverage distilled human expertise to invent new games, and foster new connections.

---

The Bitter Lesson: http://www.incompleteideas.net/IncIdeas/BitterLesson.html

---

Comment here: https://bsky.app/profile/did:plc:ktgsgyyog2ukxp4may7aiar3/post/3mbkjzttt622t