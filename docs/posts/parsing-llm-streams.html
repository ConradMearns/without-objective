<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parsing LLM Output Streams - Without Objective</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <header>
        <h1><a href="../index.html">Without Objective</a></h1>
        <p class="tagline">wip</p>
    </header>

    <div class="container">
        <nav id="post-list">
            <h2>Posts</h2>
            <div id="nav-posts">
                <!-- Nav loaded by JavaScript from manifest.json -->
            </div>
        </nav>

        <main id="content">
            <!-- Introduction: -->
<h1>Introduction</h1>
<ul>
<li>.txt discusses an interesting technique for forcing valid outputs to be generated by LLM's in <a href="https://blog.dottxt.co/how-fast-cfg.html">How fast can grammar-structured generation be?</a></li>
<li>I don't have good enough computers to manipulate the output layer, so I'm stuck with OpenAI API compatible solutions.</li>
<li>This blog outlines some of my thoughts and observations while guiding Cline through development of <a href="https://github.com/ConradMearns/pocket_parsing">this experiment</a>.</li>
<li>Most of this document is an afterthought - I hope to clarify this process for myself more as I go on.</li>
</ul>
<!-- Motivation: -->
<h1>Motivation</h1>
<ul>
<li>Cline and other Agentic coding models are likely to stick around, even without any significant advances in LLM performance.</li>
</ul>
<!--
Loose IBIS notation

`#` heading 1 for hypothesis statement
`*` unordered list for actions taken / position
`-` unordered list for negative observations
`+` unordered list for positive observations
`>` Block-quote for an aside, or comment. A distraction. Start a new file to investigate?
-->

<hr />
<!-- Hypothesis: -->
<h1>Can we parse the code output of an LLM agent while it's streaming to validate correctness closer to real-time?</h1>
<!-- Actions: -->
<ul>
<li>Started with Pocketflow Streaming with Interrupt Example</li>
<li>Added tree-sitter-python</li>
<li>Modified StreamNode to feed characters into accumulator for iterative validation</li>
</ul>
<!-- Observations: -->
<ul>
<li>
<p>Incremental parsing triggers errors too early
    Found that when we trigger an interrupt whenever a parsing error is detected, certain codes with error when maybe they should not.</p>
<p><code>python
x = 42</code></p>
<p>This will error, because as it flows through the system, at some point we will only have</p>
<p><code>python
x =</code></p>
<p>Which is not valid python. The behavior of our system is correct - but our assumptions about the problem are not.</p>
</li>
</ul>
<!-- Hypothesis: -->
<h1>Can we mitigate [[- Incremental parsing triggers errors too early|this behavior]] by counting errors over time?</h1>
<p>For example, when an erroneous error like this is detected, it is likely to be resolved soon. When resolved, the error count will go back to 0. Perhaps an error threshold will work because when errors arise, if they are erroneous they will disappear before many errors accumulate.</p>
<ul>
<li>Added the error accumulator.</li>
<li>Still had problems after 5 errors were detected</li>
<li>Increased threshold to 300.</li>
<li>Added feature to print error count continuously for more observations.</li>
<li>Many, many errors. Decided to change approach.</li>
<li>
<p>Added new tool to plot AST errors as a text is incrementally parsed character by character.</p>
<p>Ingest and plot plot.py to see how AST errors accumulate and resolve over time. plot.py contains no syntax errors, as it was used to produce it's own plot.</p>
<p><img alt="Error Count During Python Code Streaming" src="error_plot.svg" /></p>
<blockquote>
<p>A cool plotting tool for demonstrating this would be to show the text side by side with the chart and present a slider.
As the slider spans from 0 to the EOF, you could print the text, AST, and graph. </p>
</blockquote>
</li>
<li>
<p>The spikes in error counts are likely from multi-line text blocks.</p>
</li>
<li>This is just how Tree-sitter works.
    We probably need to use something else that handles incremental parsing differently.
    Ideally, we need a system that knows the difference between a parsing error, and a syntax error that occurs from out-of-language tokens.</li>
</ul>
<h1>Could <a href="https://github.com/lark-parser/lark/tree/master">lark</a> work for incremental parsing?</h1>
<ul>
<li>Create small_lark.py for testing.</li>
<li>Expected tokens are listed in the exception, so we can detect errors that end-of-input related or not.</li>
<li>Not perfect, <code>Example: x = 42</code> passes.</li>
<li><code>def x():\n\ta=1</code> fails</li>
<li>Added more newline's to the lark examples - they seem to work as expected now.</li>
</ul>
<hr />
<p>There's some useful potential for wrapping parsers directly around streamed outputs for better evaluation. Usually, Cline just makes a best attempt and then fails, executes the bad code, and parses the errors. This is not the direction I take as a programmer.</p>
<p>Documentation access is also usually ignored as a step - both by Cline and the model itself (even when documentation is explicitly provided)</p>
        </main>
    </div>

    <footer>
        <p>Built with Typst | <a href="https://github.com">Source</a></p>
    </footer>

    <script src="../js/nav.js"></script>
    <script src="../js/comments.js"></script>
    <script>
        // Initialize nav and comments on page load
        document.addEventListener('DOMContentLoaded', () => {
            loadNav('parsing-llm-streams');
            initComments('parsing-llm-streams', 'ecfeab4');
        });
    </script>
</body>
</html>